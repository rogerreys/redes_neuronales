{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "Overfit: Es cuando la red se aprende los datos\n",
    "Underfit: Cuando el modelo no esta listo para resolver problemas\n",
    "\n",
    "<img src=\"src/img/overfitting.png\" width=\"500px\"/>\n",
    "\n",
    "# Regularizacion\n",
    "Hacer los datos mas regulares, teoria de Occam's Razor\n",
    "\n",
    "## Teoria de Occam's Razor\n",
    "Si se tiene dos posibles soluciones, se escoje la que menos problemas tenga.\n",
    "\n",
    "## Regularizacion en RN\n",
    "Para lograr esto se debe reducir la complejidad en el modelo, se debe reducir los pesos\n",
    "\n",
    "### Calculo\n",
    "**L1 Regularizacion**\n",
    "$$Cost = \\sum^N_{i=0}(y_i - \\sum^M_{j=0}x_{ij}*W_j)^2 + \\lambda \\sum^M_{j=0}|W_j|$$\n",
    "\n",
    "**L2 Regularizacion**\n",
    "$$Cost = \\sum^N_{i=0}(y_i - \\sum^M_{j=0}x_{ij}*W_j)^2 + \\lambda \\sum^M_{j=0}W_j^2$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "Loss function: $\\sum^N_{i=0}(y_i - \\sum^M_{j=0}x_{ij}*W_j)^2$\n",
    "\n",
    "Regularization Term: $\\lambda \\sum^M_{j=0}W_j^2$\n",
    "\n",
    "Si $\\delta$ es muy bajo no le esta sumando peso y no se castiga\n",
    "Si $\\delta$ es muy alto no va a converger en el descenso del gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "Apaga ciertas neuronas en la red neuronal de manera aleatoria, para que una neurona no dependa tando de otra neurona y no haya una conspiracion entre neuronas\n",
    "\n",
    "<img src=\"src/img/dropout.png\" width=\"600px\">\n",
    "<img src=\"src/img/dropout_rn.png\" width=\"600px\">\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
