{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura \n",
    "\n",
    "<img src=\"src/img/arquitectura.png\" width=\"800px\">\n",
    "\n",
    "Aquí hay algunos elementos clave de la arquitectura de las redes neuronales:\n",
    "\n",
    "1. **Neuronas y capas:** Las redes neuronales están compuestas por capas de neuronas. La capa de entrada recibe los datos de entrada y transmite la información a través de una o más capas ocultas hacia la capa de salida. Cada neurona en una capa se conecta con las neuronas de la capa siguiente a través de conexiones llamadas pesos.\n",
    "\n",
    "2. **Pesos y funciones de activación:** Los pesos son valores asociados a las conexiones entre neuronas y representan la fuerza o importancia de la conexión. Cada neurona realiza una suma ponderada de las entradas que recibe y aplica una función de activación para determinar su salida. La función de activación introduce no linealidades en la red y permite que esta aprenda relaciones complejas entre los datos.\n",
    "\n",
    "3. **Feedforward y retropropagación:** En una red neuronal de alimentación directa (feedforward), la información fluye en una dirección, desde la capa de entrada hasta la capa de salida. Durante el entrenamiento, se utiliza un algoritmo llamado retropropagación (backpropagation) para ajustar los pesos de las conexiones en función del error entre las salidas deseadas y las salidas reales de la red. La retropropagación calcula las derivadas parciales del error con respecto a los pesos, permitiendo que la red se adapte y mejore su rendimiento.\n",
    "\n",
    "4. **Arquitecturas especializadas:** Además de las redes neuronales de alimentación directa, existen otras arquitecturas especializadas. Algunas de las más comunes son las redes neuronales recurrentes (RNN)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectores\n",
    "\n",
    "<img src=\"src/img/rn_vectorial.png\" width=\"500px\">\n",
    "\n",
    "La forma de calcular es por producto punto \n",
    "\n",
    "<img src=\"src/img/rn_vectorial_calculo.png\" width=\"500px\">\n",
    "\n",
    "Resultado\n",
    "\n",
    "<img src=\"src/img/rn_vectorial_res.png\" width=\"500px\">\n",
    "\n",
    "En las redes neuronales no se van a manejar vectores, sino matrices\n",
    "\n",
    "<img src=\"src/img/rn_matrices.png\" width=\"500px\">\n",
    "\n",
    "Para este caso las filas y las columnas tienen que tener una relacion con la otra matriz\n",
    "\n",
    "<img src=\"src/img/rn_matrices_ejem.png\" width=\"500px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activaciones\n",
    "Nose puede apilar sumatirias de funciones lineales en una arquitectura de red neuronal, porque el resultado seria una ecuacion lineal. Para ello se usan funciones de activacion, existen dos:\n",
    "\n",
    "- Discretas\n",
    "- Continuas\n",
    "\n",
    "<img src=\"src/img/fun_act.png\" width=\"700px\">\n",
    "\n",
    "La funcion AND utiliza una funcion de activacion discreta\n",
    "\n",
    "<img src=\"src/img/fun_act_and_discreta.png\" width=\"700px\">\n",
    "\n",
    "### Funcion escalon / escalonada / threshold\n",
    "Es muy util para tener dos salidas\n",
    "\n",
    "<img src=\"src/img/fun_act_escalon.png\" width=\"700px\">\n",
    "\n",
    "### Funcion signo / signum\n",
    "\n",
    "<img src=\"src/img/fun_act_signo.png\" width=\"700px\">\n",
    "\n",
    "### Funcion sigmoidal / sigmoid\n",
    "\n",
    "<img src=\"src/img/fun_act_sigmoid.png\" width=\"700px\">\n",
    "\n",
    "### Funcion tangente hiperbolica/tahn\n",
    "\n",
    "Es similar al sigmoidal pero va de -1 a 1\n",
    "\n",
    "<img src=\"src/img/fun_act_tanh.png\" width=\"700px\">\n",
    "\n",
    "### Funcion lineal rectificada / ReLu\n",
    "Si es menor a 0 se mantiene en cero, si es mayor a cero crece al infinito\n",
    "\n",
    "<img src=\"src/img/fun_act_relu.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "### Funcion Softmax\n",
    "\n",
    "Da la probabilidad de cada una de las posibles salidas.\n",
    "Se usa para hacer clasificacion binaria y multiple\n",
    "\n",
    "La función toma como entrada un vector de valores reales y produce como salida un vector de la misma dimensión, donde cada elemento del vector de salida representa la probabilidad de que la entrada pertenezca a una de las clases posibles.\n",
    "\n",
    "La función softmax se define matemáticamente de la siguiente manera:\n",
    "\n",
    "$$softmax(x_i) = \\frac{exp(x_i)}{sum(exp(x_j))}$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "**x_i →** es el i-ésimo elemento del vector de entrada.\n",
    "\n",
    "**exp(x_i) →** es la función exponencial de x_i, que calcula el valor e elevado a la potencia de x_i.\n",
    "\n",
    "**sum(exp(x_j)) →** es la suma de las funciones exponenciales de todos los elementos del vector de entrada.\n",
    "\n",
    "La función softmax realiza dos pasos principales:\n",
    "\n",
    "1. Calcula la función exponencial de cada elemento del vector de entrada, lo que asegura que los valores resultantes sean siempre positivos.\n",
    "2. Normaliza los valores exponenciales dividiéndolos por la suma de todos ellos, lo que garantiza que la suma de las probabilidades sea igual a 1.\n",
    "\n",
    "Al aplicar la función softmax, los elementos más grandes del vector de entrada se vuelven aún más grandes en el vector de salida, mientras que los elementos más pequeños se vuelven aún más pequeños. Esto amplifica las diferencias entre los valores y resalta las probabilidades relativas de las diferentes clases.\n",
    "\n",
    "<img src=\"src/img/fun_softmax.png\" width=\"700px\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
