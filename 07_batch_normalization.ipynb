{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "\n",
    "Batch Normalization, a menudo abreviado como \"BatchNorm\", es una técnica introducida en el aprendizaje profundo y el machine learning para mejorar la velocidad, el rendimiento y la estabilidad del entrenamiento de redes neuronales.\n",
    "\n",
    "Fue presentado por Sergey Ioffe y Christian Szegedy en 2015\n",
    "En esencia, Batch Normalization realiza lo siguiente:\n",
    "\n",
    "`1. Normalización:` Durante el entrenamiento, después de calcular la salida de una capa específica y antes de aplicar una función de activación, BatchNorm normaliza las activaciones (o sea, las salidas de la capa) de manera que tengan una media de cero y una varianza de uno. Esta normalización se realiza por cada característica y se basa en el mini-lote actual, de ahí el nombre \"Batch Normalization\".\n",
    "\n",
    "`2. Escala y Cambio:` BatchNorm introduce dos parámetros aprendibles por característica: un `parámetro de escala (γ)` y un `parámetro de cambio (β)`. Estos parámetros permiten que el modelo decida si realmente la normalización es beneficiosa o si debe escalar y desplazar la distribución normalizada a una diferente.\n",
    "\n",
    "`3. Durante el proceso de entrenamiento`, BatchNorm también `mantiene una estimación móvil de la media` y la varianza de cada característica. Estas estimaciones se utilizan en lugar de las del mini-lote durante la evaluación o inferencia para garantizar que la normalización sea coherente, independientemente del tamaño del lote.\n",
    "\n",
    "La idea detrás de BatchNorm es `reducir el cambio interno de covariables`. Es decir, trata de abordar el problema donde la distribución de las activaciones cambia durante el entrenamiento debido a los cambios en los parámetros de las capas anteriores. \n",
    "Al mantener las activaciones en una distribución más consistente, \n",
    "- BatchNorm tiende a permitir tasas de aprendizaje más altas\n",
    "- Reduce la necesidad de inicializadores específicos y actúa en cierta medida como regularizador, \n",
    "- A menudo reduciendo la necesidad de Dropout u otras técnicas de regularización."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
