{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "\n",
    "Batch Normalization, a menudo abreviado como \"BatchNorm\", es una técnica introducida en el aprendizaje profundo y el machine learning para mejorar la velocidad, el rendimiento y la estabilidad del entrenamiento de redes neuronales.\n",
    "\n",
    "Fue presentado por Sergey Ioffe y Christian Szegedy en 2015\n",
    "En esencia, Batch Normalization realiza lo siguiente:\n",
    "\n",
    "`1. Normalización:` Durante el entrenamiento, después de calcular la salida de una capa específica y antes de aplicar una función de activación, BatchNorm normaliza las activaciones (o sea, las salidas de la capa) de manera que tengan una media de cero y una varianza de uno. Esta normalización se realiza por cada característica y se basa en el mini-lote actual, de ahí el nombre \"Batch Normalization\".\n",
    "\n",
    "`2. Escala y Cambio:` BatchNorm introduce dos parámetros aprendibles por característica: un `parámetro de escala (γ)` y un `parámetro de cambio (β)`. Estos parámetros permiten que el modelo decida si realmente la normalización es beneficiosa o si debe escalar y desplazar la distribución normalizada a una diferente.\n",
    "\n",
    "`3. Durante el proceso de entrenamiento`, BatchNorm también `mantiene una estimación móvil de la media` y la varianza de cada característica. Estas estimaciones se utilizan en lugar de las del mini-lote durante la evaluación o inferencia para garantizar que la normalización sea coherente, independientemente del tamaño del lote.\n",
    "\n",
    "La idea detrás de BatchNorm es `reducir el cambio interno de covariables`. Es decir, trata de abordar el problema donde la distribución de las activaciones cambia durante el entrenamiento debido a los cambios en los parámetros de las capas anteriores. \n",
    "Al mantener las activaciones en una distribución más consistente, \n",
    "- BatchNorm tiende a permitir tasas de aprendizaje más altas\n",
    "- Reduce la necesidad de inicializadores específicos y actúa en cierta medida como regularizador, \n",
    "- A menudo reduciendo la necesidad de Dropout u otras técnicas de regularización.\n",
    "\n",
    "<img src=\"./src/img/normalization.png\" width=\"700px\">\n",
    "\n",
    "<img src=\"./src/img/BatchNorm.png\" width=\"700px\">\n",
    "\n",
    "## Matematicas en batch normalization\n",
    "\n",
    "<img src=\"./src/img/matematicas_batchNorm.png\" width=\"700px\">\n",
    "\n",
    "## Codigo con Batch Normalization\n",
    "\n",
    "```python\n",
    "model.add(Conv2D(base_hidden_units, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization()) # Batch Normalization\n",
    "\n",
    "# CONV2\n",
    "model.add(Conv2D(base_hidden_units, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization()) # Batch Normalization\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
