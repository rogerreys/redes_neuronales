{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Problem type|Last-layer activation|Loss function|\n",
    "|---|---|---|\n",
    "|Binary classification|sigmoid|binary_crossentropy|\n",
    "|Multiclass, single-label classification|softmax|categorical_crossentropy|\n",
    "|Multiclass, mutilabel classification|sigmoid|binary_crossentropy|\n",
    "|Regression to arbitrary values|None|mse|\n",
    "|Regression to values between 0 and 1|sigmoid|mse or binary_crossentropy|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mas por aprender\n",
    "- CNN (Convolutional Neural Network)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2340/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" width=\"500px\">\n",
    "\n",
    "- NLP (Natural Language Process)\n",
    "\n",
    "<img src=\"https://i.gifer.com/O0L2.gif\" width=\"500px\">\n",
    "\n",
    "- Sequences\n",
    "- Hyper parameters tuning\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/0*HFHpywPyj81DBX86.gif\" width=\"500px\">\n",
    "\n",
    "- Gans (Generative Adversarial NetworkS)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/640/0*xuZJA7BHYhVtg4Vf.gif\" width=\"500px\">\n",
    "\n",
    "- LSTM (Long Short-Term Memory)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2494/1*KxrxyB10ZbOc3xjDneQdhA.gif\" width=\"500px\">\n",
    "\n",
    "- Callbacks\n",
    "- Tensorboard\n",
    "\n",
    "<img src=\"https://spinorlab.matrix.jp/en/wp-content/uploads/2019/09/tensorboard1.gif\" width=\"500px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen\n",
    "\n",
    "1. Keras es un API que facilita el trabajo de crear redes neuronales, Keras usa el siguiente backend como su fuente:\n",
    "\n",
    "`RES: Tensorflow`\n",
    "\n",
    "2. Es uno de los grandes problemas de trabajar con deep learning\n",
    "\n",
    "`RES: Overfitting`\n",
    "\n",
    "3. ¿En qué consiste el aprendizaje “profundo”?\n",
    "\n",
    "`RES: Las capas reciben información de la capa anterior, las procesan y el resultado lo van pasando a las siguientes capas, haciendo un aprendizaje más detallado cuanto más capas existan en la red.`\n",
    "\n",
    "4. ¿Cuál es la función del BIAS en la neurona?\n",
    "\n",
    "`RES: Dar más flexibilidad al modelo desplazando la respuesta lineal en los ejes`\n",
    "\n",
    "5. ¿En qué consisten las sumas ponderadas de la neurona?\n",
    "\n",
    "`RES: Realizar un producto punto entre los datos de entrada y los pesos asociados a la neurona.`\n",
    "\n",
    "6. Es uno de los problemas que una neurona o perceptrón no puede solucionar, por sus propias limitantes\n",
    "\n",
    "`RES: XOR`\n",
    "\n",
    "7. En una red neuronal las primeras capas obtienen la información más general de los datos y las últimas capas información más específica o detallada. Esta afirmación es:\n",
    "\n",
    "`RES: Verdadera`\n",
    "\n",
    "8. ¿Por qué son necesarias las funciones de activación?\n",
    "\n",
    "`RES: Porque cada neurona devuelve una función lineal al igual que la suma de funciones lineales da como resultado otra función lineal, de ser así se obtiene el mismo resultado y es necesario deformar la función lineal.`\n",
    "\n",
    "\n",
    "11. Es una función de activación muy útil que también cuenta con derivada y funciona perfecto cuando queremos resolver un problema de probabilidad binaria:\n",
    "\n",
    "`RES: Sigmoid`\n",
    "\n",
    "12. ¿Cuál es el objetivo de la función de pérdida?\n",
    "\n",
    "`RES: Identificar numéricamente el error o diferencia entre los valores predichos y los reales.`\n",
    "\n",
    "13. ¿Es posible derivar la función de pérdida?\n",
    "\n",
    "`RES: Verdadero`\n",
    "\n",
    "14. ¿Cuál es el principal objetivo del descenso del gradiente?\n",
    "\n",
    "`RES: Encontrar paso a paso el mínimo global en la función de pérdida`\n",
    "\n",
    "\n",
    "15. Podemos decir que el learning rate \"LP\" es:\n",
    "\n",
    "`RES: Un delta que me indica el tamaño de los “pasos” que se aplican al descenso del gradiente en cada iteración.`\n",
    "\n",
    "\n",
    "16. Uno de los conceptos inspirados en la física para evitar que el descenso del gradiente se estanque en un mínimo local es:\n",
    "\n",
    "`ERROR: Masa * Aceleración`\n",
    "\n",
    "17. Una de las desventajas de tener un learning rate LR muy bajo es:\n",
    "\n",
    "`RES: El entrenamiento de la red neuronal será más lento.`\n",
    "\n",
    "\n",
    "18. El objetivo del backpropagation es:\n",
    "\n",
    "`RES: Distribuir el error a través de toda la red neuronal para actualizar sus pesos de acuerdo a dicho error.`\n",
    "\n",
    "19. En numpy (np) podemos usar esta función para realizar un producto punto\n",
    "\n",
    "`RES: matmul`\n",
    "\n",
    "20. Como buena práctica en deep learning, ¿en cuántos sub sets de datos es mejor dividir nuestros datos?\n",
    "\n",
    "`RES: 3`\n",
    "\n",
    "21. La función de pérdida recomendada para resolver un problema de clasificación binaria es:\n",
    "\n",
    "`RES: Binary_crossentropy`\n",
    "\n",
    "22. ¿Qué función de activación en la última capa es recomendable para un problema de clasificación binaria?\n",
    "\n",
    "`RES: Sigmoid`\n",
    "\n",
    "23. La función de pérdida recomendada para resolver un problema de clasificación múltiple es:\n",
    "\n",
    "`RES: categorical_crossentropy`\n",
    "\n",
    "\n",
    "24. ¿Qué función de activación en la última capa es recomendable para un problema de clasificación múltiple?\n",
    "\n",
    "`ERROR: Sigmoid`\n",
    "`SOL: Softmax`\n",
    "\n",
    "25. La función de pérdida recomendada para resolver un problema de regresión con valores arbitrarios es:\n",
    "\n",
    "`RES: MSE`\n",
    "\n",
    "26. ¿Qué función de activación en la última capa es recomendable para un problema de regresión?\n",
    "\n",
    "`ERROR: Sigmoid`\n",
    "`SOL: None`\n",
    "\n",
    "27. Podemos clasificar a sets de datos de dos dimensiones como:\n",
    "\n",
    "`RES: Matrices`\n",
    "\n",
    "28. La función de pérdida Cross Entropy funciona mejor con problemas de:\n",
    "\n",
    "`RES: Clasificación`\n",
    "\n",
    "29. La regularización L1 consiste en incrementar el valor de la función de pérdida basándonos en:\n",
    "\n",
    "`RES: Los valores absolutos de los pesos de la red`\n",
    "\n",
    "30. La capa de dropout consiste en:\n",
    "\n",
    "`RES: Dejar en 0 aleatoriamente una fracción de las neuronas en la salida de la capa.`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
