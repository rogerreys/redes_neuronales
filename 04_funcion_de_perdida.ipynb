{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCION DE PERDIDA\n",
    "\n",
    "Las funciones de pérdida, también conocidas como funciones de costo o funciones objetivo, son utilizadas en las redes neuronales para medir la discrepancia o error entre las predicciones del modelo y los valores reales de los datos de entrenamiento\n",
    "\n",
    "<img src=\"src/img/lost.png\" width=\"600px\">\n",
    "\n",
    "## Error cuadrático medio (MSE, Mean Squared Error):\n",
    "Esta función de pérdida es ampliamente utilizada en problemas de regresión. **Mide la media de los errores al cuadrado entre las predicciones y los valores reales**. Su fórmula es:\n",
    "|||\n",
    "|---|---|\n",
    "|$$MSE = \\frac{1}{n} * \\sum(y_{pred} - y_{actual})^2$$ <br> Donde: <br>**n** es el número de muestras.<br> **y_pred** es la predicción del modelo. <br>**y_actual** es el valor real correspondiente.| <img src=\"src/img/lost_mse.png\" width=\"600px\">|\n",
    "\n",
    "## Binary Cross-entropy\n",
    "\n",
    "Entropía cruzada binaria (Binary Cross-Entropy): Esta función de pérdida **se utiliza cuando se realiza una clasificación binaria**, es decir, **cuando se tiene una salida con dos clases posibles (0 o 1)**. Mide la divergencia entre las distribuciones de probabilidad predichas y las distribuciones reales. Su fórmula es:\n",
    "|||\n",
    "|---|---|\n",
    "|$$BinaryCrossEntropy = -\\sum(y_{actual} * \\log(y_{pred}) + (1 - y_{actual}) * \\log(1 - y_{pred}))$$ <br> Donde: <br> **y_pred** es la predicción del modelo. <br> **y_actual** es el valor real correspondiente.|<img src=\"src/img/lost_cross_entropy.png\" width=\"600px\">|\n",
    "\n",
    "## Categorical Cross-Entropy\n",
    "Entropía cruzada categórica (Categorical Cross-Entropy): **Esta función de pérdida se utiliza en problemas de clasificación con más de dos clases**. **Mide la discrepancia entre las distribuciones de probabilidad predichas y las distribuciones reales**. Su fórmula es:\n",
    "\n",
    "$$Categorical_Cross-Entropy = -\\sum(y_{actual} * \\log(y_{pred}))$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "**y_pred →** es la predicción del modelo.\n",
    "\n",
    "**y_actual →** es el vector de valores reales correspondientes a la distribución categórica (one-hot encoding)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
